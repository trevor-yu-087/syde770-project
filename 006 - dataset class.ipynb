{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartwatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, valid_files, sample_period=0.02):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        valid_files: list of filepaths to normalized data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for file in valid_files:\n",
    "            df = pd.read_csv(file)\n",
    "            # Resample the data if needed\n",
    "            df.index = pd.to_timedelta(df[\"time\"], unit=\"seconds\")\n",
    "            df = df.drop(\"time\", axis=1)\n",
    "            df = df.resample(f\"{sample_period}S\").mean()\n",
    "            self.data.append(df.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns tuple of (data, label) at index\"\"\"\n",
    "        item = self.data[index]\n",
    "        inputs = item[:, 0:9]  # IMU sensor data [accel, mag, gyro]\n",
    "        labels = item[:, 9:]  # Mocap data [pos, quat]\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SmartwatchAugment:\n",
    "    \"\"\"\n",
    "    Collate function to apply random augmentations to the data\n",
    "        - Randomly perturb the mocap positions\n",
    "        - Randomly flip sign of mocap quaternion\n",
    "        - Add random noise to IMU channels\n",
    "    \"\"\"\n",
    "    def __init__(self, position_noise=0.2, accel_eps=0.01, gyro_eps=0.01, mag_eps=0.01, max_samples=512):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        position_noise: float, limits on uniform distribution [-p, p] to add position offset to mocap\n",
    "        accel_eps: float, standard deviation on Gaussian noise added to accelerometer channels\n",
    "        gyro_eps: float, standard deviation on Gaussian noise added to gyroscope channels\n",
    "        mag_eps: float, standard deviation on Gaussian noise added to mangetometer channels\n",
    "        \"\"\"\n",
    "        self.position_noise = position_noise\n",
    "        self.accel_eps = accel_eps\n",
    "        self.gyro_eps = gyro_eps\n",
    "        self.mag_eps = mag_eps\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def _random_crop(self, inputs, labels):\n",
    "        \"\"\"Apply a random crop of the signal of length self.max_samples to both inputs and labels, if able to\"\"\"\n",
    "        n, d = inputs.shape\n",
    "        max_offset = n - self.max_samples\n",
    "\n",
    "        if max_offset > 0:\n",
    "            offset = rng.choice(max_offset)\n",
    "            inds = slice(offset, offset + self.max_samples)\n",
    "            return inputs[inds, :], labels[inds, :]\n",
    "        else:\n",
    "            return inputs, labels\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: list of tuple of (inputs, labels) of length batch_size\n",
    "            inputs: np.ndarray, dimensions (n_samples, 9), signal data for IMU accel, gyro, and mag\n",
    "            labels: np.ndarray, dimensions (n_samples, 7), position and quaternion data from mocap\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        (inputs, labels): augmented signal data, augmented labels\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        y = []\n",
    "        for (inputs, labels) in data:\n",
    "            inputs, labels = self._random_crop(inputs, labels)\n",
    "\n",
    "            n_in, d_in = inputs.shape\n",
    "            n_out, d_out = labels.shape\n",
    "            assert n_in == n_out, \"Inputs and outputs must have the same number of sequence elements\"\n",
    "            assert d_in == 9, f\"Input has dimensionality {d_in} instead of 9\"\n",
    "            assert d_out == 7, f\"Output has dimensionality {d_out} instead of 7\"\n",
    "\n",
    "            # Augment XYZ positions\n",
    "            offset = rng.uniform(-self.position_noise, self.position_noise, size=(1, 3))\n",
    "            labels[:, 0:3] += offset\n",
    "            # Augment quaternion sign\n",
    "            sign = rng.choice([-1, 1])\n",
    "            labels[:, 4:] *= sign\n",
    "\n",
    "            accel_noise = rng.normal(loc=0, scale=self.accel_eps, size=(n_in, 3))\n",
    "            gyro_noise = rng.normal(loc=0, scale=self.gyro_eps, size=(n_in, 3))\n",
    "            mag_noise = rng.normal(loc=0, scale=self.mag_eps, size=(n_in, 3))\n",
    "\n",
    "            noise = np.hstack([accel_noise, gyro_noise, mag_noise])\n",
    "            inputs += noise\n",
    "\n",
    "            x.append(torch.FloatTensor(inputs))\n",
    "            y.append(torch.FloatTensor(labels))\n",
    "        lengths = [len(item) for item in x]\n",
    "        inds = np.flip(np.argsort(lengths)).copy() # PackedSequence expects lengths from longest to shortest\n",
    "        lengths = torch.LongTensor(lengths)[inds]\n",
    "\n",
    "        # Sort by lengths\n",
    "        x = [x[i] for i in inds]\n",
    "        y = [y[i] for i in inds]\n",
    "\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_sequence(x)\n",
    "        packed_labels = torch.nn.utils.rnn.pack_sequence(y)\n",
    "\n",
    "        return packed_inputs, packed_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "valid_files = glob.glob(\"/root/data/smartwatch/subjects/*/*_full.csv\")\n",
    "len(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmartwatchDataset(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=SmartwatchAugment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[  8.4448,  -2.3273,   2.5778,  ...,   1.2733,  -0.8953,   0.1584],\n",
       "        [  1.5007,  -9.7277,   0.3826,  ...,  -0.6069,   1.4073,   0.4584],\n",
       "        [  2.0592, -10.4292,   8.6233,  ...,  -0.2093,   1.3515,   1.0180],\n",
       "        ...,\n",
       "        [  8.7372,  -3.7298,   1.3926,  ...,  -0.1982,   0.0597,  -0.1112],\n",
       "        [  2.2426,  -9.8952,   1.5674,  ...,  -0.2384,   1.2790,  -0.1377],\n",
       "        [  3.5819,  -3.8354,   5.9943,  ...,   0.1273,   2.0623,   1.6066]]), batch_sizes=tensor([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=9, hidden_size=32, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = lstm(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, (hidden, cell) = outputs\n",
    "output.data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
