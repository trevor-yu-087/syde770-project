{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartwatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, valid_files, sample_period=0.02):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        valid_files: list of filepaths to normalized data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for file in valid_files:\n",
    "            df = pd.read_csv(file)\n",
    "            # Resample the data if needed\n",
    "            df.index = pd.to_timedelta(df[\"time\"], unit=\"seconds\")\n",
    "            df = df.drop(\"time\", axis=1)\n",
    "            df = df.resample(f\"{sample_period}S\").mean()\n",
    "            self.data.append(df.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns tuple of (data, label) at index\"\"\"\n",
    "        item = self.data[index]\n",
    "        imu = item[:, 0:9]  # IMU sensor data [accel, mag, gyro]\n",
    "        mocap = item[:, 9:]  # Mocap data [pos, quat]\n",
    "        return imu, mocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartwatchAugment:\n",
    "    \"\"\"\n",
    "    Collate function to apply random augmentations to the data\n",
    "        - Randomly perturb the mocap positions\n",
    "        - Randomly flip sign of mocap quaternion\n",
    "        - Add random noise to IMU channels\n",
    "    \"\"\"\n",
    "    def __init__(self, position_noise=0.2, accel_eps=0.01, gyro_eps=0.01, mag_eps=0.01, max_samples=512):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        position_noise: float, limits on uniform distribution [-p, p] to add position offset to mocap\n",
    "        accel_eps: float, standard deviation on Gaussian noise added to accelerometer channels\n",
    "        gyro_eps: float, standard deviation on Gaussian noise added to gyroscope channels\n",
    "        mag_eps: float, standard deviation on Gaussian noise added to mangetometer channels\n",
    "        \"\"\"\n",
    "        self.position_noise = position_noise\n",
    "        self.accel_eps = accel_eps\n",
    "        self.gyro_eps = gyro_eps\n",
    "        self.mag_eps = mag_eps\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def _random_crop(self, imu, mocap):\n",
    "        \"\"\"\n",
    "        Apply a random crop of the signal of length self.max_samples to both inputs and labels, if able to\n",
    "        Due to targets being a shifted version of decoder inputs, we need to account for one extra timepoint\n",
    "        \"\"\"\n",
    "        n, d = imu.shape\n",
    "        max_offset = n - self.max_samples - 1\n",
    "\n",
    "        if max_offset > 0:\n",
    "            offset = rng.choice(max_offset)\n",
    "            inds = slice(offset, offset + self.max_samples + 1)\n",
    "            return imu[inds, :], mocap[inds, :]\n",
    "        else:\n",
    "            return imu, mocap\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: list of tuple of (imu, mocap) of length batch_size\n",
    "            imu: np.ndarray, dimensions (n_samples, 9), signal data for IMU accel, gyro, and mag\n",
    "            mocap: np.ndarray, dimensions (n_samples, 7), position and quaternion data from mocap\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        collated_data: dict of torch.nn.utils.rnn.PackedSequence with keys [\"encoder_inputs\", \"decoder_inputs\", \"targets\"]\n",
    "        \"\"\"\n",
    "        encoder_inputs = []\n",
    "        decoder_inputs = []\n",
    "        targets = []\n",
    "        for (imu, mocap) in data:\n",
    "            imu, mocap = self._random_crop(imu, mocap)\n",
    "\n",
    "            n_in, d_in = imu.shape\n",
    "            n_out, d_out = mocap.shape\n",
    "            assert n_in == n_out, \"IMU and mocap must have the same number of sequence elements\"\n",
    "            assert d_in == 9, f\"IMU data has dimensionality {d_in} instead of 9\"\n",
    "            assert d_out == 7, f\"Mocap data has dimensionality {d_out} instead of 7\"\n",
    "\n",
    "            # Augment XYZ positions\n",
    "            offset = rng.uniform(-self.position_noise, self.position_noise, size=(1, 3))\n",
    "            mocap[:, 0:3] += offset\n",
    "            # Augment quaternion sign\n",
    "            sign = rng.choice([-1, 1])\n",
    "            mocap[:, 4:] *= sign\n",
    "\n",
    "            accel_noise = rng.normal(loc=0, scale=self.accel_eps, size=(n_in, 3))\n",
    "            gyro_noise = rng.normal(loc=0, scale=self.gyro_eps, size=(n_in, 3))\n",
    "            mag_noise = rng.normal(loc=0, scale=self.mag_eps, size=(n_in, 3))\n",
    "\n",
    "            noise = np.hstack([accel_noise, gyro_noise, mag_noise])\n",
    "            imu += noise\n",
    "\n",
    "            # Ensure targets are one timestep shifted wrt inputs\n",
    "            encoder_inputs.append(torch.FloatTensor(imu[:-1, :]))\n",
    "            decoder_inputs.append(torch.FloatTensor(mocap[:-1, :]))\n",
    "            targets.append(torch.FloatTensor(mocap[1:, :]))\n",
    "\n",
    "        lengths = [len(item) for item in encoder_inputs]\n",
    "        inds = np.flip(np.argsort(lengths)).copy()  # PackedSequence expects lengths from longest to shortest\n",
    "        lengths = torch.LongTensor(lengths)[inds]\n",
    "\n",
    "        # Sort by lengths\n",
    "        encoder_inputs = [encoder_inputs[i] for i in inds]\n",
    "        decoder_inputs = [decoder_inputs[i] for i in inds]\n",
    "        targets = [targets[i] for i in inds]\n",
    "\n",
    "        # encoder_inputs = torch.nn.utils.rnn.pack_sequence(encoder_inputs)\n",
    "        # decoder_inputs = torch.nn.utils.rnn.pack_sequence(decoder_inputs)\n",
    "        # targets = torch.nn.utils.rnn.pack_sequence(targets)\n",
    "        collated_data = {\n",
    "            \"encoder_inputs\": encoder_inputs,\n",
    "            \"decoder_inputs\": decoder_inputs,\n",
    "            \"targets\": targets\n",
    "        }\n",
    "        return collated_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "valid_files = glob.glob(\"/root/data/smartwatch/subjects/*/*_full.csv\")\n",
    "len(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects = [f\"S{n}\" for n in [5, 10, 15, 20, 25, 30]]\n",
    "test_files = [file for file in valid_files for subject in test_subjects if f\"/{subject}/\" in file]\n",
    "train_files = [file for file in valid_files if file not in set(test_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 576)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_files), len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmartwatchDataset(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=SmartwatchAugment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = batch[\"encoder_inputs\"]\n",
    "dec = batch[\"decoder_inputs\"]\n",
    "tgt = batch[\"targets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 7]), torch.Size([512, 7]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0].shape, tgt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.7261, -0.7642, -0.8087]), tensor([-0.7642, -0.8087, -0.8533]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0][:3, 0], tgt[0][:3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=9, hidden_size=32, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k: torch.nn.utils.rnn.pack_sequence(v) for k, v in batch.items()}\n",
    "\n",
    "outputs = lstm(batch[\"encoder_inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, (hidden, cell) = outputs\n",
    "output.data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
