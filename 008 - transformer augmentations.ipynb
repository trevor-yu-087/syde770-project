{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following this tutorial: https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\n",
    "# Modified for PyTorch\n",
    "\n",
    "#class TransformerAugmentations():\n",
    "def padding_mask(input, pad_idx=0):\n",
    "    # Create mask which marks the zero padding values in the input by a 1\n",
    "    mask = input == pad_idx\n",
    "    #mask = mask.float()\n",
    "\n",
    "    return mask\n",
    "\n",
    "def lookahead_mask(shape):\n",
    "    # Mask out future entries by marking them with a 1.0\n",
    "    mask = 1 - torch.tril(torch.ones((shape, shape)))\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    " \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Padding mask for encoder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m enc_padding_mask \u001b[39m=\u001b[39m padding_mask(encoder_input)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Padding and look-ahead masks for decoder\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dec_in_padding_mask \u001b[39m=\u001b[39m padding_mask(decoder_input)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_input' is not defined"
     ]
    }
   ],
   "source": [
    "# Padding mask for encoder\n",
    "enc_padding_mask = padding_mask(encoder_input)\n",
    "\n",
    "# Padding and look-ahead masks for decoder\n",
    "dec_in_padding_mask = padding_mask(decoder_input)\n",
    "dec_in_lookahead_mask = lookahead_mask(decoder_input.shape[1])\n",
    "dec_in_lookahead_mask = torch.maximum(dec_in_padding_mask, dec_in_lookahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
      "torch.Size([10])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "shape = 10\n",
    "test_input = torch.ones((shape))\n",
    "test_input[5:] = 0\n",
    "p_mask = padding_mask(test_input)\n",
    "print(p_mask)\n",
    "print(p_mask.shape)\n",
    "l_mask = lookahead_mask(shape)\n",
    "print(l_mask)\n",
    "print(test_input * p_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "\n",
    "def Transformer_train_fn(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        encoder_model,\n",
    "        decoder_model,\n",
    "        encoder_optimizer,\n",
    "        decoder_optimizer,\n",
    "        loss_fn,\n",
    "        metric_loss_fn,\n",
    "        num_epoch,\n",
    "        device,\n",
    "        save_path,\n",
    "        writer,\n",
    "        teacher_force_ratio=1,\n",
    "        val_interval=100000,\n",
    "        checkpoint=None,\n",
    "):\n",
    "    for epoch in range(num_epoch):\n",
    "        print(f'===== Epoch: {epoch} =====')\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_metric = 0\n",
    "\n",
    "        for train_step, train_data in enumerate(train_loader):\n",
    "            train_source = train_data['encoder_inputs'].to(device)\n",
    "            train_target = train_data['decoder_inputs'].to(device)\n",
    "\n",
    "            # Zero optimizers\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            decoder_output = torch.zeros(4, 512, 7).to(device)\n",
    "            train_target_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(train_target, batch_first=True)\n",
    "            train_target_unpacked.to(device)\n",
    "            start = train_target_unpacked[:, 0, :].unsqueeze(1).to(device)\n",
    "            teacher_force = True if random.random() < teacher_force_ratio else False\n",
    "\n",
    "            encoder_hidden, encoder_cell = encoder_model(train_source)\n",
    "            # print(encoder_hidden.shape)\n",
    "            encoder_cell = torch.zeros(1, 4, 32).to(device)\n",
    "            \n",
    "            if train_step == 0:\n",
    "                decoder_output, decoder_hidden, decoder_cell = decoder_model(train_target, encoder_hidden, encoder_cell)\n",
    "                # print(f'Decoder Output: {decoder_output.shape}\\t Decoder Hidden: {decoder_hidden.shape}\\t Decoder Cell: {decoder_cell.shape}')\n",
    "            elif train_step !=0 and teacher_force == True:\n",
    "                decoder_output, decoder_hidden, decoder_cell = decoder_model(train_target, encoder_hidden, encoder_cell)\n",
    "            elif train_step != 0 and teacher_force == False:\n",
    "                for i in range(1, 512):\n",
    "                    start = torch.nn.utils.rnn.pack_sequence(start)\n",
    "                    decoder_output[:, i, :], decoder_hidden, decoder_cell = decoder_model(start, encoder_hidden, encoder_cell)\n",
    "                    start = train_target_unpacked[:, i, :].unsqueeze(1)\n",
    "                    encoder_hidden = decoder_hidden\n",
    "                    encoder_cell = decoder_cell\n",
    "\n",
    "            train_loss = loss_fn(decoder_output, train_target_unpacked)\n",
    "\n",
    "            # Backwards\n",
    "            train_loss.backward()\n",
    "\n",
    "            # Update optimizers\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            # Train loss\n",
    "            epoch_train_loss += train_loss.item()\n",
    "\n",
    "            # Train metric loss\n",
    "            train_metric = metric_loss_fn(decoder_output, train_target_unpacked)\n",
    "            epoch_train_metric += train_metric\n",
    "\n",
    "        # Average losses for tensorboard\n",
    "        epoch_train_loss /= (train_step+1)\n",
    "        writer.add_scalar('Training MSE per Epoch', epoch_train_loss, epoch)\n",
    "        epoch_train_metric /= (train_step+1)\n",
    "        writer.add_scalar('Training MAE per Epoch', epoch_train_metric, epoch)\n",
    "        \n",
    "\n",
    "        if epoch+1 % val_interval == 0:\n",
    "            encoder_model.eval()\n",
    "            decoder_model.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_val_loss = 0\n",
    "                epoch_val_metric = 0\n",
    "\n",
    "                for val_step, val_data in enumerate(val_loader):\n",
    "                    val_source = val_data['encoder_inputs'].to(device)\n",
    "                    val_target = val_data['decoder_inputs'].to(device)\n",
    "\n",
    "                    # Run validation model\n",
    "                    val_encoder_hidden, val_encoder_cell = encoder_model(val_source)\n",
    "                    val_decoder_output, val_decoder_hidden, val_decoder_cell = decoder_model(val_target, val_encoder_hidden, val_encoder_cell)\n",
    "\n",
    "                    val_loss = loss_fn(val_decoder_output, val_target)\n",
    "\n",
    "                    # Val loss\n",
    "                    epoch_val_loss += val_loss.item()\n",
    "\n",
    "                    # Val metric loss\n",
    "                    val_metric = metric_loss_fn(val_decoder_output, val_target)\n",
    "                    epoch_val_metric += val_metric\n",
    "\n",
    "                # Average validation losses for tensorboard\n",
    "                epoch_val_loss /= (val_step+1)\n",
    "                writer.add_scalar('Validation MSE per Epoch', epoch_val_loss, epoch)\n",
    "                epoch_val_metric /= (val_step+1)\n",
    "                writer.add_scalar('Validation MAE per Epoch', epoch_val_metric, epoch)\n",
    "\n",
    "\n",
    "                 # Save checkpoint\n",
    "                if not os.path.exists(os.path.join(save_path, 'checkpoint')):\n",
    "                    os.makedirs(os.path.join(save_path, 'checkpoint'))\n",
    "                torch.save({'epoch': epoch,\n",
    "                            'encoder_model_state_dict': encoder_model.state_dict(),\n",
    "                            'decoder_model_state_dict': decoder_model.state_dict(),\n",
    "                            'encoder_optim_state_dict': encoder_optimizer.state_dict(),\n",
    "                            'decoder_optim_state_dict': decoder_optimizer.state_dict(),\n",
    "                            'train_loss': epoch_train_loss,\n",
    "                            'val_loss': epoch_val_loss},\n",
    "                           os.path.join(save_path, 'checkpoint', 'checkpoint_{}.pth'.format(epoch))\n",
    "                           )\n",
    "                \n",
    "                # Save best model\n",
    "                if not os.path.exists(os.path.join(save_path, 'best')):\n",
    "                    os.makedirs(os.path.join(save_path, 'best'))\n",
    "                if epoch_val_metric > best_metric:\n",
    "                    best_metric = epoch_val_metric\n",
    "                    best_metric_epoch = epoch\n",
    "                    torch.save(encoder_model.state_dict(), os.path.join(save_path, 'best', 'best_encoder_model.pth'))\n",
    "                    torch.save(decoder_model.state_dict(), os.path.join(save_path, 'best', 'best_decoder_model.pth'))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "valid_files = glob.glob(\"/root/data/smartwatch/subjects/*/*_full.csv\")\n",
    "len(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SmartwatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, valid_files, sample_period=0.02):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        valid_files: list of filepaths to normalized data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for file in valid_files:\n",
    "            df = pd.read_csv(file)\n",
    "            # Resample the data if needed\n",
    "            df.index = pd.to_timedelta(df[\"time\"], unit=\"seconds\")\n",
    "            df = df.drop(\"time\", axis=1)\n",
    "            df = df.resample(f\"{sample_period}S\").mean()\n",
    "            self.data.append(df.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns tuple of (imu, mocap) at index\"\"\"\n",
    "        item = self.data[index]\n",
    "        imu = item[:, 0:9]  # IMU sensor data [accel, mag, gyro]\n",
    "        mocap = item[:, 9:]  # Mocap data [pos, quat]\n",
    "        return imu, mocap\n",
    "    \n",
    "\n",
    "class SmartwatchAugmentTransformer:\n",
    "    \"\"\"\n",
    "    Collate function to apply random augmentations to the data\n",
    "        - Randomly perturb the mocap positions\n",
    "        - Randomly flip sign of mocap quaternion\n",
    "        - Add random noise to IMU channels\n",
    "        - Random crop to the signal (if possible)\n",
    "    \"\"\"\n",
    "    def __init__(self, position_noise=0.2, accel_eps=0.01, gyro_eps=0.01, mag_eps=0.01, max_samples=512):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        position_noise: float, limits on uniform distribution [-p, p] to add position offset to mocap\n",
    "        accel_eps: float, standard deviation on Gaussian noise added to accelerometer channels\n",
    "        gyro_eps: float, standard deviation on Gaussian noise added to gyroscope channels\n",
    "        mag_eps: float, standard deviation on Gaussian noise added to mangetometer channels\n",
    "        \"\"\"\n",
    "        self.position_noise = position_noise\n",
    "        self.accel_eps = accel_eps\n",
    "        self.gyro_eps = gyro_eps\n",
    "        self.mag_eps = mag_eps\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def _random_crop(self, imu, mocap):\n",
    "        \"\"\"\n",
    "        Apply a random crop of the signal of length self.max_samples to both inputs and labels, if able to\n",
    "        Due to targets being a shifted version of decoder inputs, we need to account for one extra timepoint\n",
    "        \"\"\"\n",
    "        n, d = imu.shape\n",
    "        max_offset = n - self.max_samples - 1\n",
    "\n",
    "        if max_offset > 0:\n",
    "            offset = rng.choice(max_offset)\n",
    "            inds = slice(offset, offset + self.max_samples + 1)\n",
    "            return imu[inds, :], mocap[inds, :]\n",
    "        else:\n",
    "            return imu, mocap\n",
    "        \n",
    "\n",
    "    def padding_mask(self, input, pad_idx=0, dim=512): \n",
    "        # Create mask which marks the zero padding values in the input by a 0\n",
    "        mask = torch.zeros((dim))\n",
    "        if input.shape[0] < dim:\n",
    "            mask[input.shape[0]:] = 1\n",
    "            return mask.bool()\n",
    "        #mask = mask.float()\n",
    "\n",
    "        return mask.bool()\n",
    "\n",
    "\n",
    "    def lookahead_mask(self, shape):\n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - torch.tril(torch.ones((shape, shape)))\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    \n",
    "        return mask\n",
    "\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: list of tuple of (imu, mocap) of length batch_size\n",
    "            imu: np.ndarray, dimensions (n_samples, 9), signal data for IMU accel, gyro, and mag\n",
    "            mocap: np.ndarray, dimensions (n_samples, 7), position and quaternion data from mocap\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        collated_data: dict of torch.nn.utils.rnn.PackedSequence with keys [\"encoder_inputs\", \"decoder_inputs\", \"targets\"]\n",
    "        \"\"\"\n",
    "        encoder_inputs = []\n",
    "        decoder_inputs = []\n",
    "        targets = []\n",
    "        for (imu, mocap) in data:\n",
    "            imu, mocap = self._random_crop(imu, mocap)\n",
    "\n",
    "            n_in, d_in = imu.shape\n",
    "            n_out, d_out = mocap.shape\n",
    "            assert n_in == n_out, \"IMU and mocap must have the same number of sequence elements\"\n",
    "            assert d_in == 9, f\"IMU data has dimensionality {d_in} instead of 9\"\n",
    "            assert d_out == 7, f\"Mocap data has dimensionality {d_out} instead of 7\"\n",
    "\n",
    "            # Augment XYZ positions\n",
    "            offset = rng.uniform(-self.position_noise, self.position_noise, size=(1, 3))\n",
    "            mocap[:, 0:3] += offset\n",
    "            # Augment quaternion sign\n",
    "            sign = rng.choice([-1, 1])\n",
    "            mocap[:, 4:] *= sign\n",
    "\n",
    "            accel_noise = rng.normal(loc=0, scale=self.accel_eps, size=(n_in, 3))\n",
    "            gyro_noise = rng.normal(loc=0, scale=self.gyro_eps, size=(n_in, 3))\n",
    "            mag_noise = rng.normal(loc=0, scale=self.mag_eps, size=(n_in, 3))\n",
    "\n",
    "            noise = np.hstack([accel_noise, gyro_noise, mag_noise])\n",
    "            imu += noise\n",
    "\n",
    "            # Ensure targets are one timestep shifted wrt inputs\n",
    "            encoder_inputs.append(torch.FloatTensor(imu[:-1, :]))\n",
    "            decoder_inputs.append(torch.FloatTensor(mocap[:-1, :]))\n",
    "            targets.append(torch.FloatTensor(mocap[1:, :]))\n",
    "\n",
    "        lengths = [len(item) for item in encoder_inputs]\n",
    "        inds = np.flip(np.argsort(lengths)).copy()  # PackedSequence expects lengths from longest to shortest\n",
    "        lengths = torch.LongTensor(lengths)[inds]\n",
    "\n",
    "        # Sort by lengths\n",
    "        encoder_inputs = [encoder_inputs[i] for i in inds]\n",
    "        decoder_inputs = [decoder_inputs[i] for i in inds]\n",
    "        targets = [targets[i] for i in inds]\n",
    "\n",
    "        # Padding mask for encoder\n",
    "        enc_padding_mask = [self.padding_mask(input=encoder_inputs[i]) for i in inds]\n",
    "        enc_lookahead_mask = [self.lookahead_mask(shape=encoder_inputs[i].shape[1]) for i in inds]\n",
    "        \n",
    "        # Padding and look-ahead masks for decoder\n",
    "        dec_in_padding_mask = [self.padding_mask(input=decoder_inputs[i]) for i in inds]\n",
    "        dec_in_lookahead_mask = [self.lookahead_mask(shape=decoder_inputs[i].shape[1]) for i in inds]\n",
    "        #dec_in_lookahead_mask = [torch.maximum(dec_in_padding_mask[i], dec_in_lookahead_mask[i]) for i in inds]\n",
    "\n",
    "        # Pad input, if needed\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length != 512:\n",
    "                print(\"Dim does not equal 512 - padding sequence\") \n",
    "                encoder_inputs[i] = nn.functional.pad(encoder_inputs[i], pad=(0, 512 - encoder_inputs[i].shape[0]), mode='constant', value=0)\n",
    "                decoder_inputs[i] = nn.functional.pad(decoder_inputs[i], pad=(0, 512 - decoder_inputs[i].shape[0]), mode='constant', value=0)\n",
    "                targets[i] = nn.functional.pad(targets[i], pad=(0, 512 - targets[i].shape[0]), mode='constant', value=0)\n",
    "\n",
    "        encoder_inputs = torch.stack(encoder_inputs)\n",
    "        decoder_inputs = torch.stack(decoder_inputs)\n",
    "        targets = torch.stack(targets)\n",
    "\n",
    "        enc_padding_mask = torch.stack(enc_padding_mask)\n",
    "        enc_lookahead_mask = torch.stack(enc_lookahead_mask)\n",
    "        dec_in_padding_mask = torch.stack(dec_in_padding_mask)\n",
    "        dec_in_lookahead_mask = torch.stack(dec_in_lookahead_mask)\n",
    "\n",
    "        collated_data = {\n",
    "            \"encoder_inputs\": encoder_inputs,\n",
    "            \"decoder_inputs\": decoder_inputs,\n",
    "            \"targets\": targets,\n",
    "            \"encoder_padding_mask\": enc_padding_mask,\n",
    "            \"decoder_padding_mask\": dec_in_padding_mask,\n",
    "            \"decoder_lookahead_mask\": dec_in_lookahead_mask,\n",
    "            \"encoder_lookahead_mask\": enc_lookahead_mask\n",
    "        }\n",
    "        return collated_data\n",
    "\n",
    "def get_file_lists():\n",
    "    \"\"\"Get list of files to pass to dataset class\n",
    "    Returns:\n",
    "    --------\n",
    "    train_files: list of str filepaths to pre-processed train data\n",
    "    test_files: list of str filepaths to pre-processed test data\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    valid_files = glob.glob(\"/root/data/smartwatch/subjects/*/*_full.csv\")\n",
    "    test_subjects = [f\"S{n}\" for n in [5, 10, 15, 20, 25, 30]]\n",
    "    test_files = [file for file in valid_files for subject in test_subjects if f\"/{subject}/\" in file]\n",
    "    train_files = [file for file in valid_files if file not in set(test_files)]\n",
    "    return train_files, test_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmartwatchDataset(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=SmartwatchAugmentTransformer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['encoder_padding_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 9])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 9, 9])\n",
      "torch.Size([16, 512, 7])\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(batch['encoder_inputs'].data.shape)\n",
    "print(batch['encoder_padding_mask'].data.shape)\n",
    "print(batch['encoder_lookahead_mask'].data.shape)\n",
    "print(batch['decoder_inputs'].data.shape)\n",
    "print(batch['decoder_padding_mask'].data.shape)\n",
    "print(batch['decoder_lookahead_mask'].data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "batch[0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=9, hidden_size=32, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = lstm(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, (hidden, cell) = outputs\n",
    "output.data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
