{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CNN Tranformer Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "import model.hyperparameters as hp\n",
    "from model.Transformer import TransformerModel\n",
    "from utils.train import Transformer_train_fn\n",
    "from utils.dataset import (\n",
    "    SmartwatchDataset, \n",
    "    SmartwatchAugmentTransformer, \n",
    "    get_file_lists\n",
    ")\n",
    "from utils.utils import test_Transformer\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 06:25:38.211297: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-16 06:25:38.211406: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-04-16 06:25:38.211419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "SAVE_PATH = Path(f'outputs/{datetime.now().strftime(\"%d-%m-%Y_%H%M%S\")}')\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN == True:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(log_dir=f'{SAVE_PATH}/tensorboard')\n",
    "    TEST_PATH = SAVE_PATH\n",
    "else:\n",
    "    TEST_PATH = Path(input('Enter path to folder containing weights: '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get .csv files\n",
    "train_files, val_files, test_files = get_file_lists(\n",
    "    val_sub_list=['05', 10, 15, 20, 25, 30],\n",
    "    test_sub_list=[35],\n",
    ")\n",
    "\n",
    "# Get dataloaders\n",
    "train_dataset = SmartwatchDataset(train_files, sample_period=0.04)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hp.TRANSFORMER_BATCH_SIZE, collate_fn=SmartwatchAugmentTransformer(num_heads=hp.NUM_HEADS), drop_last=True, shuffle=True)\n",
    "\n",
    "val_dataset = SmartwatchDataset(val_files)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hp.TRANSFORMER_BATCH_SIZE, collate_fn=SmartwatchAugmentTransformer(num_heads=hp.NUM_HEADS), drop_last=True, shuffle=True)\n",
    "\n",
    "test_dataset = SmartwatchDataset(test_files)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hp.TRANSFORMER_BATCH_SIZE, collate_fn=SmartwatchAugmentTransformer(num_heads=hp.NUM_HEADS), drop_last=True, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer\n",
    "transformer_model = TransformerModel(\n",
    "    input_size=9,\n",
    "    stride=2,\n",
    "    kernel_size=32,\n",
    "    seq_len=512,\n",
    "    channels=[32],\n",
    "    dropout=0.1,\n",
    "    n_heads=hp.NUM_HEADS,\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1,\n",
    "    downsample=False\n",
    ").to(hp.DEVICE)\n",
    "\n",
    "# Initialize loss functions\n",
    "loss_fn = nn.MSELoss()\n",
    "metric_loss_fn = nn.L1Loss()\n",
    "\n",
    "# Initialize optimizers\n",
    "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=hp.TRANSFORMER_LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Epoch: 0 =====\n",
      "torch.Size([4, 512, 9])\n",
      "torch.Size([4, 512, 7])\n",
      "torch.Size([4, 512, 7])\n",
      "For i is 1, torch.Size([4, 512, 7])\n",
      "For i is 2, torch.Size([4, 512, 7])\n",
      "For i is 3, torch.Size([4, 512, 7])\n",
      "For i is 4, torch.Size([4, 512, 7])\n",
      "For i is 5, torch.Size([4, 512, 7])\n",
      "For i is 6, torch.Size([4, 512, 7])\n",
      "For i is 7, torch.Size([4, 512, 7])\n",
      "For i is 8, torch.Size([4, 512, 7])\n",
      "For i is 9, torch.Size([4, 512, 7])\n",
      "For i is 10, torch.Size([4, 512, 7])\n",
      "For i is 11, torch.Size([4, 512, 7])\n",
      "For i is 12, torch.Size([4, 512, 7])\n",
      "For i is 13, torch.Size([4, 512, 7])\n",
      "For i is 14, torch.Size([4, 512, 7])\n",
      "For i is 15, torch.Size([4, 512, 7])\n",
      "For i is 16, torch.Size([4, 512, 7])\n",
      "For i is 17, torch.Size([4, 512, 7])\n",
      "For i is 18, torch.Size([4, 512, 7])\n",
      "For i is 19, torch.Size([4, 512, 7])\n",
      "For i is 20, torch.Size([4, 512, 7])\n",
      "For i is 21, torch.Size([4, 512, 7])\n",
      "For i is 22, torch.Size([4, 512, 7])\n",
      "For i is 23, torch.Size([4, 512, 7])\n",
      "For i is 24, torch.Size([4, 512, 7])\n",
      "For i is 25, torch.Size([4, 512, 7])\n",
      "For i is 26, torch.Size([4, 512, 7])\n",
      "For i is 27, torch.Size([4, 512, 7])\n",
      "For i is 28, torch.Size([4, 512, 7])\n",
      "For i is 29, torch.Size([4, 512, 7])\n",
      "For i is 30, torch.Size([4, 512, 7])\n",
      "For i is 31, torch.Size([4, 512, 7])\n",
      "For i is 32, torch.Size([4, 512, 7])\n",
      "For i is 33, torch.Size([4, 512, 7])\n",
      "For i is 34, torch.Size([4, 512, 7])\n",
      "For i is 35, torch.Size([4, 512, 7])\n",
      "For i is 36, torch.Size([4, 512, 7])\n",
      "For i is 37, torch.Size([4, 512, 7])\n",
      "For i is 38, torch.Size([4, 512, 7])\n",
      "For i is 39, torch.Size([4, 512, 7])\n",
      "For i is 40, torch.Size([4, 512, 7])\n",
      "For i is 41, torch.Size([4, 512, 7])\n",
      "For i is 42, torch.Size([4, 512, 7])\n",
      "For i is 43, torch.Size([4, 512, 7])\n",
      "For i is 44, torch.Size([4, 512, 7])\n",
      "For i is 45, torch.Size([4, 512, 7])\n",
      "For i is 46, torch.Size([4, 512, 7])\n",
      "For i is 47, torch.Size([4, 512, 7])\n",
      "For i is 48, torch.Size([4, 512, 7])\n",
      "For i is 49, torch.Size([4, 512, 7])\n",
      "For i is 50, torch.Size([4, 512, 7])\n",
      "For i is 51, torch.Size([4, 512, 7])\n",
      "For i is 52, torch.Size([4, 512, 7])\n",
      "For i is 53, torch.Size([4, 512, 7])\n",
      "For i is 54, torch.Size([4, 512, 7])\n",
      "For i is 55, torch.Size([4, 512, 7])\n",
      "For i is 56, torch.Size([4, 512, 7])\n",
      "For i is 57, torch.Size([4, 512, 7])\n",
      "For i is 58, torch.Size([4, 512, 7])\n",
      "For i is 59, torch.Size([4, 512, 7])\n",
      "For i is 60, torch.Size([4, 512, 7])\n",
      "For i is 61, torch.Size([4, 512, 7])\n",
      "For i is 62, torch.Size([4, 512, 7])\n",
      "For i is 63, torch.Size([4, 512, 7])\n",
      "For i is 64, torch.Size([4, 512, 7])\n",
      "For i is 65, torch.Size([4, 512, 7])\n",
      "For i is 66, torch.Size([4, 512, 7])\n",
      "For i is 67, torch.Size([4, 512, 7])\n",
      "For i is 68, torch.Size([4, 512, 7])\n",
      "For i is 69, torch.Size([4, 512, 7])\n",
      "For i is 70, torch.Size([4, 512, 7])\n",
      "For i is 71, torch.Size([4, 512, 7])\n",
      "For i is 72, torch.Size([4, 512, 7])\n",
      "For i is 73, torch.Size([4, 512, 7])\n",
      "For i is 74, torch.Size([4, 512, 7])\n",
      "For i is 75, torch.Size([4, 512, 7])\n",
      "For i is 76, torch.Size([4, 512, 7])\n",
      "For i is 77, torch.Size([4, 512, 7])\n",
      "For i is 78, torch.Size([4, 512, 7])\n",
      "For i is 79, torch.Size([4, 512, 7])\n",
      "For i is 80, torch.Size([4, 512, 7])\n",
      "For i is 81, torch.Size([4, 512, 7])\n",
      "For i is 82, torch.Size([4, 512, 7])\n",
      "For i is 83, torch.Size([4, 512, 7])\n",
      "For i is 84, torch.Size([4, 512, 7])\n",
      "For i is 85, torch.Size([4, 512, 7])\n",
      "For i is 86, torch.Size([4, 512, 7])\n",
      "For i is 87, torch.Size([4, 512, 7])\n",
      "For i is 88, torch.Size([4, 512, 7])\n",
      "For i is 89, torch.Size([4, 512, 7])\n",
      "For i is 90, torch.Size([4, 512, 7])\n",
      "For i is 91, torch.Size([4, 512, 7])\n",
      "For i is 92, torch.Size([4, 512, 7])\n",
      "For i is 93, torch.Size([4, 512, 7])\n",
      "For i is 94, torch.Size([4, 512, 7])\n",
      "For i is 95, torch.Size([4, 512, 7])\n",
      "For i is 96, torch.Size([4, 512, 7])\n",
      "For i is 97, torch.Size([4, 512, 7])\n",
      "For i is 98, torch.Size([4, 512, 7])\n",
      "For i is 99, torch.Size([4, 512, 7])\n",
      "For i is 100, torch.Size([4, 512, 7])\n",
      "For i is 101, torch.Size([4, 512, 7])\n",
      "For i is 102, torch.Size([4, 512, 7])\n",
      "For i is 103, torch.Size([4, 512, 7])\n",
      "For i is 104, torch.Size([4, 512, 7])\n",
      "For i is 105, torch.Size([4, 512, 7])\n",
      "For i is 106, torch.Size([4, 512, 7])\n",
      "For i is 107, torch.Size([4, 512, 7])\n",
      "For i is 108, torch.Size([4, 512, 7])\n",
      "For i is 109, torch.Size([4, 512, 7])\n",
      "For i is 110, torch.Size([4, 512, 7])\n",
      "For i is 111, torch.Size([4, 512, 7])\n",
      "For i is 112, torch.Size([4, 512, 7])\n",
      "For i is 113, torch.Size([4, 512, 7])\n",
      "For i is 114, torch.Size([4, 512, 7])\n",
      "For i is 115, torch.Size([4, 512, 7])\n",
      "For i is 116, torch.Size([4, 512, 7])\n",
      "For i is 117, torch.Size([4, 512, 7])\n",
      "For i is 118, torch.Size([4, 512, 7])\n",
      "For i is 119, torch.Size([4, 512, 7])\n",
      "For i is 120, torch.Size([4, 512, 7])\n",
      "For i is 121, torch.Size([4, 512, 7])\n",
      "For i is 122, torch.Size([4, 512, 7])\n",
      "For i is 123, torch.Size([4, 512, 7])\n",
      "For i is 124, torch.Size([4, 512, 7])\n",
      "For i is 125, torch.Size([4, 512, 7])\n",
      "For i is 126, torch.Size([4, 512, 7])\n",
      "For i is 127, torch.Size([4, 512, 7])\n",
      "For i is 128, torch.Size([4, 512, 7])\n",
      "For i is 129, torch.Size([4, 512, 7])\n",
      "For i is 130, torch.Size([4, 512, 7])\n",
      "For i is 131, torch.Size([4, 512, 7])\n",
      "For i is 132, torch.Size([4, 512, 7])\n",
      "For i is 133, torch.Size([4, 512, 7])\n",
      "For i is 134, torch.Size([4, 512, 7])\n",
      "For i is 135, torch.Size([4, 512, 7])\n",
      "For i is 136, torch.Size([4, 512, 7])\n",
      "For i is 137, torch.Size([4, 512, 7])\n",
      "For i is 138, torch.Size([4, 512, 7])\n",
      "For i is 139, torch.Size([4, 512, 7])\n",
      "For i is 140, torch.Size([4, 512, 7])\n",
      "For i is 141, torch.Size([4, 512, 7])\n",
      "For i is 142, torch.Size([4, 512, 7])\n",
      "For i is 143, torch.Size([4, 512, 7])\n",
      "For i is 144, torch.Size([4, 512, 7])\n",
      "For i is 145, torch.Size([4, 512, 7])\n",
      "For i is 146, torch.Size([4, 512, 7])\n",
      "For i is 147, torch.Size([4, 512, 7])\n",
      "For i is 148, torch.Size([4, 512, 7])\n",
      "For i is 149, torch.Size([4, 512, 7])\n",
      "For i is 150, torch.Size([4, 512, 7])\n",
      "For i is 151, torch.Size([4, 512, 7])\n",
      "For i is 152, torch.Size([4, 512, 7])\n",
      "For i is 153, torch.Size([4, 512, 7])\n",
      "For i is 154, torch.Size([4, 512, 7])\n",
      "For i is 155, torch.Size([4, 512, 7])\n",
      "For i is 156, torch.Size([4, 512, 7])\n",
      "For i is 157, torch.Size([4, 512, 7])\n",
      "For i is 158, torch.Size([4, 512, 7])\n",
      "For i is 159, torch.Size([4, 512, 7])\n",
      "For i is 160, torch.Size([4, 512, 7])\n",
      "For i is 161, torch.Size([4, 512, 7])\n",
      "For i is 162, torch.Size([4, 512, 7])\n",
      "For i is 163, torch.Size([4, 512, 7])\n",
      "For i is 164, torch.Size([4, 512, 7])\n",
      "For i is 165, torch.Size([4, 512, 7])\n",
      "For i is 166, torch.Size([4, 512, 7])\n",
      "For i is 167, torch.Size([4, 512, 7])\n",
      "For i is 168, torch.Size([4, 512, 7])\n",
      "For i is 169, torch.Size([4, 512, 7])\n",
      "For i is 170, torch.Size([4, 512, 7])\n",
      "For i is 171, torch.Size([4, 512, 7])\n",
      "For i is 172, torch.Size([4, 512, 7])\n",
      "For i is 173, torch.Size([4, 512, 7])\n",
      "For i is 174, torch.Size([4, 512, 7])\n",
      "For i is 175, torch.Size([4, 512, 7])\n",
      "For i is 176, torch.Size([4, 512, 7])\n",
      "For i is 177, torch.Size([4, 512, 7])\n",
      "For i is 178, torch.Size([4, 512, 7])\n",
      "For i is 179, torch.Size([4, 512, 7])\n",
      "For i is 180, torch.Size([4, 512, 7])\n",
      "For i is 181, torch.Size([4, 512, 7])\n",
      "For i is 182, torch.Size([4, 512, 7])\n",
      "For i is 183, torch.Size([4, 512, 7])\n",
      "For i is 184, torch.Size([4, 512, 7])\n",
      "For i is 185, torch.Size([4, 512, 7])\n",
      "For i is 186, torch.Size([4, 512, 7])\n",
      "For i is 187, torch.Size([4, 512, 7])\n",
      "For i is 188, torch.Size([4, 512, 7])\n",
      "For i is 189, torch.Size([4, 512, 7])\n",
      "For i is 190, torch.Size([4, 512, 7])\n",
      "For i is 191, torch.Size([4, 512, 7])\n",
      "For i is 192, torch.Size([4, 512, 7])\n",
      "For i is 193, torch.Size([4, 512, 7])\n",
      "For i is 194, torch.Size([4, 512, 7])\n",
      "For i is 195, torch.Size([4, 512, 7])\n",
      "For i is 196, torch.Size([4, 512, 7])\n",
      "For i is 197, torch.Size([4, 512, 7])\n",
      "For i is 198, torch.Size([4, 512, 7])\n",
      "For i is 199, torch.Size([4, 512, 7])\n",
      "For i is 200, torch.Size([4, 512, 7])\n",
      "For i is 201, torch.Size([4, 512, 7])\n",
      "For i is 202, torch.Size([4, 512, 7])\n",
      "For i is 203, torch.Size([4, 512, 7])\n",
      "For i is 204, torch.Size([4, 512, 7])\n",
      "For i is 205, torch.Size([4, 512, 7])\n",
      "For i is 206, torch.Size([4, 512, 7])\n",
      "For i is 207, torch.Size([4, 512, 7])\n",
      "For i is 208, torch.Size([4, 512, 7])\n",
      "For i is 209, torch.Size([4, 512, 7])\n",
      "For i is 210, torch.Size([4, 512, 7])\n",
      "For i is 211, torch.Size([4, 512, 7])\n",
      "For i is 212, torch.Size([4, 512, 7])\n",
      "For i is 213, torch.Size([4, 512, 7])\n",
      "For i is 214, torch.Size([4, 512, 7])\n",
      "For i is 215, torch.Size([4, 512, 7])\n",
      "For i is 216, torch.Size([4, 512, 7])\n",
      "For i is 217, torch.Size([4, 512, 7])\n",
      "For i is 218, torch.Size([4, 512, 7])\n",
      "For i is 219, torch.Size([4, 512, 7])\n",
      "For i is 220, torch.Size([4, 512, 7])\n",
      "For i is 221, torch.Size([4, 512, 7])\n",
      "For i is 222, torch.Size([4, 512, 7])\n",
      "For i is 223, torch.Size([4, 512, 7])\n",
      "For i is 224, torch.Size([4, 512, 7])\n",
      "For i is 225, torch.Size([4, 512, 7])\n",
      "For i is 226, torch.Size([4, 512, 7])\n",
      "For i is 227, torch.Size([4, 512, 7])\n",
      "For i is 228, torch.Size([4, 512, 7])\n",
      "For i is 229, torch.Size([4, 512, 7])\n",
      "For i is 230, torch.Size([4, 512, 7])\n",
      "For i is 231, torch.Size([4, 512, 7])\n",
      "For i is 232, torch.Size([4, 512, 7])\n",
      "For i is 233, torch.Size([4, 512, 7])\n",
      "For i is 234, torch.Size([4, 512, 7])\n",
      "For i is 235, torch.Size([4, 512, 7])\n",
      "For i is 236, torch.Size([4, 512, 7])\n",
      "For i is 237, torch.Size([4, 512, 7])\n",
      "For i is 238, torch.Size([4, 512, 7])\n",
      "For i is 239, torch.Size([4, 512, 7])\n",
      "For i is 240, torch.Size([4, 512, 7])\n",
      "For i is 241, torch.Size([4, 512, 7])\n",
      "For i is 242, torch.Size([4, 512, 7])\n",
      "For i is 243, torch.Size([4, 512, 7])\n",
      "For i is 244, torch.Size([4, 512, 7])\n",
      "For i is 245, torch.Size([4, 512, 7])\n",
      "For i is 246, torch.Size([4, 512, 7])\n",
      "For i is 247, torch.Size([4, 512, 7])\n",
      "For i is 248, torch.Size([4, 512, 7])\n",
      "For i is 249, torch.Size([4, 512, 7])\n",
      "For i is 250, torch.Size([4, 512, 7])\n",
      "For i is 251, torch.Size([4, 512, 7])\n",
      "For i is 252, torch.Size([4, 512, 7])\n",
      "For i is 253, torch.Size([4, 512, 7])\n",
      "For i is 254, torch.Size([4, 512, 7])\n",
      "For i is 255, torch.Size([4, 512, 7])\n",
      "For i is 256, torch.Size([4, 512, 7])\n",
      "For i is 257, torch.Size([4, 512, 7])\n",
      "For i is 258, torch.Size([4, 512, 7])\n",
      "For i is 259, torch.Size([4, 512, 7])\n",
      "For i is 260, torch.Size([4, 512, 7])\n",
      "For i is 261, torch.Size([4, 512, 7])\n",
      "For i is 262, torch.Size([4, 512, 7])\n",
      "For i is 263, torch.Size([4, 512, 7])\n",
      "For i is 264, torch.Size([4, 512, 7])\n",
      "For i is 265, torch.Size([4, 512, 7])\n",
      "For i is 266, torch.Size([4, 512, 7])\n",
      "For i is 267, torch.Size([4, 512, 7])\n",
      "For i is 268, torch.Size([4, 512, 7])\n",
      "For i is 269, torch.Size([4, 512, 7])\n",
      "For i is 270, torch.Size([4, 512, 7])\n",
      "For i is 271, torch.Size([4, 512, 7])\n",
      "For i is 272, torch.Size([4, 512, 7])\n",
      "For i is 273, torch.Size([4, 512, 7])\n",
      "For i is 274, torch.Size([4, 512, 7])\n",
      "For i is 275, torch.Size([4, 512, 7])\n",
      "For i is 276, torch.Size([4, 512, 7])\n",
      "For i is 277, torch.Size([4, 512, 7])\n",
      "For i is 278, torch.Size([4, 512, 7])\n",
      "For i is 279, torch.Size([4, 512, 7])\n",
      "For i is 280, torch.Size([4, 512, 7])\n",
      "For i is 281, torch.Size([4, 512, 7])\n",
      "For i is 282, torch.Size([4, 512, 7])\n",
      "For i is 283, torch.Size([4, 512, 7])\n",
      "For i is 284, torch.Size([4, 512, 7])\n",
      "For i is 285, torch.Size([4, 512, 7])\n",
      "For i is 286, torch.Size([4, 512, 7])\n",
      "For i is 287, torch.Size([4, 512, 7])\n",
      "For i is 288, torch.Size([4, 512, 7])\n",
      "For i is 289, torch.Size([4, 512, 7])\n",
      "For i is 290, torch.Size([4, 512, 7])\n",
      "For i is 291, torch.Size([4, 512, 7])\n",
      "For i is 292, torch.Size([4, 512, 7])\n",
      "For i is 293, torch.Size([4, 512, 7])\n",
      "For i is 294, torch.Size([4, 512, 7])\n",
      "For i is 295, torch.Size([4, 512, 7])\n",
      "For i is 296, torch.Size([4, 512, 7])\n",
      "For i is 297, torch.Size([4, 512, 7])\n",
      "For i is 298, torch.Size([4, 512, 7])\n",
      "For i is 299, torch.Size([4, 512, 7])\n",
      "For i is 300, torch.Size([4, 512, 7])\n",
      "For i is 301, torch.Size([4, 512, 7])\n",
      "For i is 302, torch.Size([4, 512, 7])\n",
      "For i is 303, torch.Size([4, 512, 7])\n",
      "For i is 304, torch.Size([4, 512, 7])\n",
      "For i is 305, torch.Size([4, 512, 7])\n",
      "For i is 306, torch.Size([4, 512, 7])\n",
      "For i is 307, torch.Size([4, 512, 7])\n",
      "For i is 308, torch.Size([4, 512, 7])\n",
      "For i is 309, torch.Size([4, 512, 7])\n",
      "For i is 310, torch.Size([4, 512, 7])\n",
      "For i is 311, torch.Size([4, 512, 7])\n",
      "For i is 312, torch.Size([4, 512, 7])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.91 GiB total capacity; 8.80 GiB already allocated; 22.06 MiB free; 8.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m TRAIN \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     Transformer_train_fn(\n\u001b[1;32m      3\u001b[0m         train_loader,\n\u001b[1;32m      4\u001b[0m         val_loader,\n\u001b[1;32m      5\u001b[0m         transformer_model,\n\u001b[1;32m      6\u001b[0m         transformer_optimizer,\n\u001b[1;32m      7\u001b[0m         loss_fn,\n\u001b[1;32m      8\u001b[0m         metric_loss_fn,\n\u001b[1;32m      9\u001b[0m         hp\u001b[39m.\u001b[39;49mNUM_EPOCH,\n\u001b[1;32m     10\u001b[0m         hp\u001b[39m.\u001b[39;49mDEVICE,\n\u001b[1;32m     11\u001b[0m         SAVE_PATH,\n\u001b[1;32m     12\u001b[0m         writer,\n\u001b[1;32m     13\u001b[0m         hp\u001b[39m.\u001b[39;49mTEACHER_FORCE_RATIO,\n\u001b[1;32m     14\u001b[0m         checkpoint\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m         batch_size\u001b[39m=\u001b[39;49mhp\u001b[39m.\u001b[39;49mTRANSFORMER_BATCH_SIZE\n\u001b[1;32m     16\u001b[0m     )\n",
      "File \u001b[0;32m~/tia/syde770-project/utils/train.py:212\u001b[0m, in \u001b[0;36mTransformer_train_fn\u001b[0;34m(train_loader, val_loader, transformer_model, transformer_optimizer, loss_fn, metric_loss_fn, num_epoch, device, save_path, writer, teacher_force_ratio, val_interval, checkpoint, batch_size)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m512\u001b[39m):\n\u001b[1;32m    211\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor i is \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mtransformer_output\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m         transformer_output[:, i, :] \u001b[39m=\u001b[39m transformer_model(src\u001b[39m=\u001b[39;49mtrain_source, tgt\u001b[39m=\u001b[39;49mstart)\n\u001b[1;32m    213\u001b[0m         start \u001b[39m=\u001b[39m train_target[:, i, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(transformer_output, train_target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tia/syde770-project/model/Transformer.py:124\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, tgt, src_padding, tgt_padding, tgt_lookahead)\u001b[0m\n\u001b[1;32m    116\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoder(tgt)\n\u001b[1;32m    118\u001b[0m \u001b[39m# we permute to obtain size (sequence length, batch_size, dim_model),\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m#src = src.permute(1, 0, 2)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m#tgt = tgt.permute(1, 0, 2)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39m# Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m transformer_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(src, tgt, tgt_mask\u001b[39m=\u001b[39;49mtgt_lookahead, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_padding, tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_padding)\n\u001b[1;32m    125\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(transformer_out)\n\u001b[1;32m    126\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m) \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[1;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    147\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[1;32m    148\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    149\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    150\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py:280\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    277\u001b[0m         src_key_padding_mask_for_layers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 280\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    283\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py:536\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_first:\n\u001b[1;32m    535\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x), src_mask, src_key_padding_mask)\n\u001b[0;32m--> 536\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ff_block(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(x))\n\u001b[1;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py:554\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ff_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x))))\n\u001b[1;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.91 GiB total capacity; 8.80 GiB already allocated; 22.06 MiB free; 8.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if TRAIN == True:\n",
    "    Transformer_train_fn(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        transformer_model,\n",
    "        transformer_optimizer,\n",
    "        loss_fn,\n",
    "        metric_loss_fn,\n",
    "        hp.NUM_EPOCH,\n",
    "        hp.DEVICE,\n",
    "        SAVE_PATH,\n",
    "        writer,\n",
    "        hp.TEACHER_FORCE_RATIO,\n",
    "        checkpoint=None,\n",
    "        batch_size=hp.TRANSFORMER_BATCH_SIZE\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Transformer(\n",
    "    test_loader,\n",
    "    transformer_model,\n",
    "    loss_fn,\n",
    "    metric_loss_fn,\n",
    "    SAVE_PATH,\n",
    "    hp.DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280231\n",
      "280231\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "print(pytorch_total_params)\n",
    "pytorch_trainable_params = sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)\n",
    "print(pytorch_trainable_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
